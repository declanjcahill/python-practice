{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping NYT Homepage for URL's to articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully requested today's paper from the NYT\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "nyt = requests.get('https://www.nytimes.com/section/todayspaper').text\n",
    "print('Successfully requested today\\'s paper from the NYT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added unique article URLs to list\n",
      "Total articles:  48\n"
     ]
    }
   ],
   "source": [
    "def unique(list1):\n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    "     \n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x.replace('\\\"',\"\"))\n",
    "    return unique_list\n",
    "\n",
    "urls = re.findall(r'href=([\"\\/\\d[\\w\\-\\/\\.]+?\\.html)\"', nyt)\n",
    "unique_urls = unique(urls)\n",
    "print('Successfully added unique article URLs to list')\n",
    "print('Total articles: ',len(unique_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing text content of each article in to a list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added each article HTML data to list\n",
      "Successfully extracted article paragraph text data to list\n"
     ]
    }
   ],
   "source": [
    "articles_html = []\n",
    "\n",
    "for i in range(len(unique_urls)):\n",
    "    article = requests.get('https://www.nytimes.com' + unique_urls[i]).text\n",
    "    articles_html.append(article)\n",
    "print('Successfully added each article HTML data to list')\n",
    "    \n",
    "articles = []\n",
    "try:\n",
    "    for i in range(len(articles_html)):\n",
    "        paras = re.findall(r'<p class=\"css-axufdj evys1bk0\">(.+?)</p>', articles_html[i])\n",
    "        paras = list(map(lambda x: re.sub(r'<[^>]+?>', '', x), paras))\n",
    "        articles.append(paras)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('Successfully extracted article paragraph text data to list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Article Text Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting all paragraphs of first article in to individual words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 paragraphs of article 1 paragraphs split into words:\n",
      "\n",
      "Paragraph 1 in sentence form: \n",
      "Follow live coverage of the Bronx apartment fire. \n",
      "\n",
      "\n",
      "Paragraph 1 in word form: \n",
      "['Follow', 'live', 'coverage', 'of', 'the', 'Bronx', 'apartment', 'fire.']\n",
      "\n",
      "\n",
      "Paragraph 2 in sentence form: \n",
      "Nineteen people, including nine children, were killed on Sunday when an apartment fire started by a malfunctioning space heater sent smoke billowing through a Bronx high-rise, officials said, in the deadliest fire New York City had seen in more than three decades.\n",
      "\n",
      "\n",
      "Paragraph 2 in word form: \n",
      "['Nineteen', 'people,', 'including', 'nine', 'children,', 'were', 'killed', 'on', 'Sunday', 'when', 'an', 'apartment', 'fire', 'started', 'by', 'a', 'malfunctioning', 'space', 'heater', 'sent', 'smoke', 'billowing', 'through', 'a', 'Bronx', 'high-rise,', 'officials', 'said,', 'in', 'the', 'deadliest', 'fire', 'New', 'York', 'City', 'had', 'seen', 'in', 'more', 'than', 'three', 'decades.']\n",
      "\n",
      "\n",
      "Paragraph 3 in sentence form: \n",
      "An additional 44 people were injured, 13 of them critically, after the occupants of the third-floor apartment where the fire started fled without closing the door behind them, the fire commissioner, Daniel A. Nigro, said at a news conference at the scene.\n",
      "\n",
      "\n",
      "Paragraph 3 in word form: \n",
      "['An', 'additional', '44', 'people', 'were', 'injured,', '13', 'of', 'them', 'critically,', 'after', 'the', 'occupants', 'of', 'the', 'third-floor', 'apartment', 'where', 'the', 'fire', 'started', 'fled', 'without', 'closing', 'the', 'door', 'behind', 'them,', 'the', 'fire', 'commissioner,', 'Daniel', 'A.', 'Nigro,', 'said', 'at', 'a', 'news', 'conference', 'at', 'the', 'scene.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print('First 3 paragraphs of article 1 paragraphs split into words:\\n')\n",
    "for parag in articles[0][:3]:\n",
    "    index += 1\n",
    "    print('Paragraph ' + str(index) + ' in sentence form: ')\n",
    "    print(parag)\n",
    "    words = parag.split()\n",
    "    print('\\n')\n",
    "    print('Paragraph ' + str(index) + ' in word form: ')\n",
    "    print(words)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning up punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "#list of punctuation symbols in library\n",
    "\n",
    "punc = string.punctuation + '’‘'\n",
    "#adding more special characters to special punctuation library\n",
    "\n",
    "def strip_punctuation(st):\n",
    "     return ''.join(i for i in st if i not in punc)\n",
    "     #returns only the characters in the string that are not in the special punctuation library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating count of each word in the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 words by number of appearances in article:\n",
      " [('the', 97), ('in', 35), ('a', 34), ('and', 31), ('fire', 30)]\n",
      "\n",
      "Unsorted wordcounts:\n",
      " {'follow': 1, 'live': 1, 'coverage': 1, 'of': 29, 'the': 97, 'bronx': 7, 'apartment': 8, 'fire': 30, 'nineteen': 1, 'people': 9, 'including': 3, 'nine': 1, 'children': 4, 'were': 13, 'killed': 2, 'on': 12, 'sunday': 6, 'when': 5, 'an': 5, 'started': 3, 'by': 13, 'a': 34, 'malfunctioning': 1, 'space': 2, 'heater': 3, 'sent': 1, 'smoke': 7, 'billowing': 1, 'through': 4, 'highrise': 1, 'officials': 4, 'said': 26, 'in': 35, 'deadliest': 2, 'new': 2, 'york': 1, 'city': 4, 'had': 7, 'seen': 1, 'more': 2, 'than': 1, 'three': 3, 'decades': 1, 'additional': 1, '44': 1, 'injured': 2, '13': 2, 'them': 5, 'critically': 1, 'after': 3, 'occupants': 1, 'thirdfloor': 2, 'where': 5, 'fled': 1, 'without': 1, 'closing': 1, 'door': 1, 'behind': 1, 'commissioner': 2, 'daniel': 1, 'nigro': 2, 'at': 10, 'news': 3, 'conference': 2, 'scene': 2, '“smoke': 1, 'spread': 2, 'throughout': 1, 'building': 14, 'thus': 1, 'tremendous': 1, 'loss': 2, 'life': 3, 'and': 31, 'other': 2, 'fighting': 1, 'for': 12, 'their': 7, 'lives”': 1, 'he': 16, 'from': 6, 'to': 28, 'top': 1, '19story': 1, 'darkening': 1, 'hallways': 1, 'stairwells': 3, 'shocking': 1, 'residents': 6, 'who': 7, 'heard': 1, 'alarms': 5, 'but': 6, 'did': 2, 'not': 4, 'immediately': 1, 'react': 1, 'because': 3, 'they': 11, 'grown': 1, 'accustomed': 1, 'frequent': 1, 'firefighters': 3, 'found': 1, 'victims': 4, 'every': 1, 'floor': 4, 'worked': 2, 'rescue': 1, 'even': 1, 'as': 5, 'own': 1, 'oxygen': 1, 'tanks': 1, 'ran': 2, 'low': 1, 'dana': 1, 'campbell': 2, 'was': 19, 'summoned': 1, 'home': 2, 'her': 3, 'four': 1, 'began': 3, 'seeping': 1, 'into': 2, 'she': 7, 'arrived': 1, 'leapt': 1, 'out': 1, 'window': 2, 'onto': 1, 'makeshift': 1, 'landing': 1, 'pad': 1, 'relieved': 1, 'see': 3, 'harmed': 1, '“you': 2, 'can': 1, 'be': 6, 'here': 3, 'tomorrow': 3, 'with': 10, 'broken': 1, 'legs”': 1, 'cant': 1, 'inhalation”': 1, 'fires': 2, 'toll': 1, 'worst': 2, 'since': 2, '87': 1, 'died': 3, 'intentionally': 1, 'set': 2, 'nightclub': 2, '1990': 2, 'early': 2, 'test': 1, 'citys': 2, 'mayor': 1, 'eric': 1, 'adams': 4, '“the': 1, 'numbers': 1, 'are': 4, 'horrific”': 1, 'mr': 6, 'first': 1, 'two': 2, 'conferences': 1, 'site': 3, 'vowed': 1, 'that': 15, 'would': 4, 'provide': 1, 'support': 2, 'many': 2, 'whom': 2, 'muslim': 1, 'immigrants': 1, 'west': 2, 'african': 3, 'nation': 1, 'gambia': 1, '“were': 1, 'all': 3, 'feeling': 1, 'this': 4, 'going': 1, 'community': 2, 'help': 5, 'navigate': 1, 'this”': 1, 'later': 1, 'returned': 1, 'thirty': 1, 'remained': 2, 'hospital': 3, 'evening': 1, 'urged': 1, 'displaced': 2, 'seek': 1, 'assured': 1, 'those': 2, 'may': 1, 'undocumented': 1, 'information': 1, 'passed': 1, 'along': 1, 'federal': 2, 'immigration': 1, 'authorities': 1, 'gov': 1, 'kathy': 1, 'hochul': 1, 'spoke': 2, 'second': 1, 'include': 1, 'funding': 1, 'assist': 1, 'cost': 1, 'housing': 3, 'burials': 1, 'budget': 1, 'proposal': 1, 'next': 1, 'week': 1, 'described': 2, 'holding': 1, 'mother': 2, 'lost': 2, 'entire': 1, 'family': 3, '“tonight': 1, 'is': 6, 'night': 2, 'tragedy': 1, 'pain': 1, 'we': 2, 'begin': 1, 'rebuild”': 1, 'official': 1, 'condition': 2, 'anonymity': 1, 'still': 1, 'under': 1, 'investigation': 1, 'marshals': 1, 'believe': 1, 'been': 1, 'running': 1, 'several': 3, 'days': 2, 'uninterrupted': 1, 'using': 1, 'supplement': 1, 'buildings': 3, 'heat': 1, 'which': 2, 'doors': 1, 'left': 2, 'open': 1, 'during': 2, 'have': 2, 'featured': 1, 'some': 3, 'blazes': 1, '2017': 1, 'dead': 1, 'young': 1, 'boy': 1, 'playing': 2, 'stove': 1, 'his': 7, 'familys': 1, 'firstfloor': 1, 'quickly': 1, 'tore': 1, 'sundays': 3, 'deadly': 1, 'occurred': 1, 'fordham': 1, 'heights': 1, 'built': 1, '1972': 1, 'it': 4, 'has': 2, 'no': 2, 'escapes': 1, 'like': 3, 'most': 1, 'modern': 1, 'highrises': 1, 'must': 1, 'rely': 1, 'event': 1, 'emergency': 1, 'surrounding': 1, 'neighborhood': 1, 'workingclass': 1, 'families': 1, 'american': 2, 'hispanic': 1, 'descent': 1, 'use': 1, 'section': 1, '8': 1, 'vouchers': 1, 'pay': 1, 'rent': 1, 'melting': 1, 'pot': 1, 'races': 1, 'religions': 1, 'languages': 1, 'sound': 1, 'alarm': 1, 'so': 1, 'common': 1, 'learned': 1, 'ignore': 1, 'ms': 1, 'lives': 2, 'third': 1, 'go': 1, 'off': 2, 'five': 1, 'or': 2, 'six': 1, 'times': 1, 'day': 1, 'do': 1, '“i': 2, 'roll': 1, 'my': 2, 'eyes”': 1, 'tower': 1, 'owned': 1, 'investors': 1, '—': 4, 'lihc': 1, 'investment': 1, 'group': 3, 'belveron': 1, 'partners': 1, 'camber': 1, 'property': 2, 'purchased': 1, 'seven': 1, 'rentregulated': 1, 'borough': 1, '166': 1, 'million': 1, '2020': 1, 'one': 3, 'cambers': 1, 'cofounders': 1, 'rick': 1, 'gropper': 1, 'adviser': 1, 'spokeswoman': 1, 'owners': 2, 'sometimes': 1, 'tripped': 1, 'smoking': 1, 'unaware': 1, 'any': 2, 'problems': 1, 'devices': 1, 'sounded': 1, 'properly': 1, '“we': 3, 'devastated': 1, 'unimaginable': 1, 'caused': 2, 'profound': 1, 'tragedy”': 1, 'statement': 1, 'happy': 2, 'land': 2, 'located': 1, 'far': 1, 'club': 2, 'operated': 1, 'illegally': 1, 'sprinklers': 1, 'exits': 1, 'blocked': 1, 'rolldown': 1, 'security': 1, 'shutters': 1, 'jilted': 1, 'lover': 1, 'whose': 1, 'former': 1, 'girlfriend': 1, 'sentenced': 1, 'prison': 1, 'while': 1, 'serving': 1, 'sentence': 1, '2016': 1, 'came': 2, 'just': 1, 'crowded': 1, 'rowhouse': 1, 'philadelphia': 1, 'dozen': 1, 'eight': 1, 'investigators': 1, 'looking': 1, 'possibility': 1, 'child': 1, 'lighter': 1, 'near': 1, 'christmas': 1, 'tree': 1, 'according': 1, 'warrant': 1, 'application': 1, 'filed': 1, 'state': 1, 'court': 1, 'sheer': 1, 'size': 1, 'guaranteed': 1, 'danger': 1, 'contains': 1, 'about': 1, '120': 1, 'units': 1, 'ranging': 1, 'studios': 1, 'fourbedroom': 1, 'apartments': 2, 'inspectors': 1, 'late': 1, 'determined': 1, 'stable': 1, 'allowed': 1, 'stay': 1, 'being': 1, 'provided': 1, 'hotel': 1, 'rooms': 1, 'red': 1, 'cross': 1, 'until': 1, 'could': 2, 'return': 1, 'find': 1, 'permanent': 1, 'interrupted': 1, 'routine': 1, 'activities': 1, 'showers': 1, 'breakfast': 1, 'coffee': 1, 'sleep': 1, 'threat': 1, 'death': 1, 'ahouss': 1, 'balima': 2, '20': 1, 'sleeping': 1, 'ninth': 1, 'awakened': 1, 'cries': 1, 'someone': 1, 'begging': 1, 'roused': 1, 'sisters': 2, 'parents': 1, 'racing': 1, 'downstairs': 1, 'stopped': 1, 'sixth': 1, 'told': 3, 'escape': 1, 'route': 1, 'too': 1, 'dangerous': 1, 'then': 1, 'toward': 1, 'room': 1, 'another': 2, 'others': 1, 'gathered': 1, 'light': 1, 'streaming': 1, 'dark': 1, 'choking': 1, 'struggle': 1, 'breathe': 1, 'sister': 2, 'cried': 1, 'screamed': 1, 'eventually': 1, 'rescued': 1, 'taken': 1, 'critical': 1, 'vomiting': 1, '“it': 1, 'really': 1, 'breaks': 1, 'heart”': 1, 'jacobi': 1, 'medical': 1, 'center': 1, 'few': 2, 'miles': 1, 'away': 2, 'ibrahim': 1, 'seece': 2, '48': 1, 'show': 1, 'tightknit': 1, 'gambian': 1, 'lived': 1, '“if': 2, 'happens': 1, 'everybody': 1, 'sympathy”': 1, 'god': 2, 'appreciate': 1, 'powerful”': 1, 'hassane': 1, 'badr': 2, '28': 1, 'brother': 2, 'searching': 1, 'dont': 2, 'know': 1, 'is”': 1, 'idea”': 1, 'feet': 1, 'man': 1, 'sobbed': 1, 'sat': 1, 'ground': 1, 'reporters': 1, 'blaze': 1, '“im': 1, 'sorry”': 1, 'television': 1, 'cameras': 1, 'shook': 1, 'white': 1, 'plastic': 1, 'bag': 1, 'filled': 1, 'belongings': 1, 'cannot': 1, 'talk”': 1}\n"
     ]
    }
   ],
   "source": [
    "wordcounts = {}                          \n",
    "for parag in articles[0][:]:          \n",
    "    words = parag.split()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = strip_punctuation(word)\n",
    "        if word in wordcounts:\n",
    "            wordcounts[word] += 1\n",
    "        else:\n",
    "            wordcounts[word] = 1\n",
    "            \n",
    "sorted_wordcounts = sorted(wordcounts.items(), key=lambda x: x[1], reverse=True)\n",
    "print('\\nTop 5 words by number of appearances in article:\\n', sorted_wordcounts[:5])\n",
    "\n",
    "print('\\nUnsorted wordcounts:\\n', wordcounts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
